{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Name - Karanjot Singh**\n",
        "\n",
        "**Roll No.- 102317275**\n",
        "\n",
        "**Subgroup-2Q11**"
      ],
      "metadata": {
        "id": "ILHmZG84nbvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports, technology, food, books, etc.).\n",
        "\n",
        "Convert text to lowercase and remove punctuation.\n",
        "\n",
        "Tokenize the text into words and sentences.\n",
        "\n",
        "Remove stopwords (using NLTK's stopwords list).\n",
        "\n",
        "Display word frequency distribution (excluding stopwords)"
      ],
      "metadata": {
        "id": "yF7_f48Nnkoi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DokubCW-58Ri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "383ea699-849a-4ec8-bda3-37e0908a9192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "technology is evolving faster than ever from smartphones and ai to space exploration and robotics every innovation is shaping the future\n",
            "it simplifies tasks connects people across the globe and drives efficiency in countless industries\n",
            "as we become more reliant on digital systems understanding technology becomes crucial\n",
            "new breakthroughs continue to amaze and transform the way we live\n",
            "the possibilities are limitless when imagination meets innovation\n",
            "['Technology is evolving faster than ever.', 'From smartphones and AI to space exploration and robotics, every innovation is shaping the future.', 'It simplifies tasks, connects people across the globe, and drives efficiency in countless industries.', 'As we become more reliant on digital systems, understanding technology becomes crucial.', 'New breakthroughs continue to amaze and transform the way we live.', 'The possibilities are limitless when imagination meets innovation.']\n",
            "['Technology', 'is', 'evolving', 'faster', 'than', 'ever', '.', 'From', 'smartphones', 'and', 'AI', 'to', 'space', 'exploration', 'and', 'robotics', ',', 'every', 'innovation', 'is', 'shaping', 'the', 'future', '.', 'It', 'simplifies', 'tasks', ',', 'connects', 'people', 'across', 'the', 'globe', ',', 'and', 'drives', 'efficiency', 'in', 'countless', 'industries', '.', 'As', 'we', 'become', 'more', 'reliant', 'on', 'digital', 'systems', ',', 'understanding', 'technology', 'becomes', 'crucial', '.', 'New', 'breakthroughs', 'continue', 'to', 'amaze', 'and', 'transform', 'the', 'way', 'we', 'live', '.', 'The', 'possibilities', 'are', 'limitless', 'when', 'imagination', 'meets', 'innovation', '.']\n",
            "['Technology', 'evolving', 'faster', 'ever', '.', 'From', 'smartphones', 'AI', 'space', 'exploration', 'robotics', ',', 'every', 'innovation', 'shaping', 'future', '.', 'It', 'simplifies', 'tasks', ',', 'connects', 'people', 'across', 'globe', ',', 'drives', 'efficiency', 'countless', 'industries', '.', 'As', 'become', 'reliant', 'digital', 'systems', ',', 'understanding', 'technology', 'becomes', 'crucial', '.', 'New', 'breakthroughs', 'continue', 'amaze', 'transform', 'way', 'live', '.', 'The', 'possibilities', 'limitless', 'imagination', 'meets', 'innovation', '.']\n",
            "Technology: 1\n",
            "evolving: 1\n",
            "faster: 1\n",
            "ever: 1\n",
            ".: 6\n",
            "From: 1\n",
            "smartphones: 1\n",
            "AI: 1\n",
            "space: 1\n",
            "exploration: 1\n",
            "robotics: 1\n",
            ",: 4\n",
            "every: 1\n",
            "innovation: 2\n",
            "shaping: 1\n",
            "future: 1\n",
            "It: 1\n",
            "simplifies: 1\n",
            "tasks: 1\n",
            "connects: 1\n",
            "people: 1\n",
            "across: 1\n",
            "globe: 1\n",
            "drives: 1\n",
            "efficiency: 1\n",
            "countless: 1\n",
            "industries: 1\n",
            "As: 1\n",
            "become: 1\n",
            "reliant: 1\n",
            "digital: 1\n",
            "systems: 1\n",
            "understanding: 1\n",
            "technology: 1\n",
            "becomes: 1\n",
            "crucial: 1\n",
            "New: 1\n",
            "breakthroughs: 1\n",
            "continue: 1\n",
            "amaze: 1\n",
            "transform: 1\n",
            "way: 1\n",
            "live: 1\n",
            "The: 1\n",
            "possibilities: 1\n",
            "limitless: 1\n",
            "imagination: 1\n",
            "meets: 1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords');\n",
        "nltk.download('punkt_tab')\n",
        "paragraph = \"\"\"Technology is evolving faster than ever. From smartphones and AI to space exploration and robotics, every innovation is shaping the future.\n",
        "It simplifies tasks, connects people across the globe, and drives efficiency in countless industries.\n",
        "As we become more reliant on digital systems, understanding technology becomes crucial.\n",
        "New breakthroughs continue to amaze and transform the way we live.\n",
        "The possibilities are limitless when imagination meets innovation.\"\"\"\n",
        "\n",
        "clean_paragraph=paragraph.lower()\n",
        "clean_paragraph=clean_paragraph.translate(str.maketrans('','',string.punctuation))\n",
        "print(clean_paragraph)\n",
        "sentence=sent_tokenize(paragraph)\n",
        "words=word_tokenize(paragraph)\n",
        "print(sentence)\n",
        "print(words)\n",
        "stop_words=set(stopwords.words('english'))\n",
        "filtered_words=[word for word in words if word not in stop_words]\n",
        "print(filtered_words)\n",
        "freq_dist = FreqDist(filtered_words)\n",
        "for word, freq in freq_dist.items():\n",
        "    print(f\"{word}: {freq}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: Stemming and Lemmatization\n",
        "\n",
        "Take the tokenized words from Question 1 (after stopword removal).\n",
        "\n",
        "Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
        "\n",
        "Apply lemmatization using NLTK's WordNetLemmatizer.\n",
        "\n",
        "Compare and display results of both techniques."
      ],
      "metadata": {
        "id": "ZVcs-Cjvobjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.stem import PorterStemmer,LancasterStemmer,WordNetLemmatizer\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "paragraph = \"\"\"Technology is evolving faster than ever. From smartphones and AI to space exploration and robotics, every innovation is shaping the future.\n",
        "It simplifies tasks, connects people across the globe, and drives efficiency in countless industries.\n",
        "As we become more reliant on digital systems, understanding technology becomes crucial.\n",
        "New breakthroughs continue to amaze and transform the way we live.\n",
        "The possibilities are limitless when imagination meets innovation.\"\"\"\n",
        "\n",
        "clean_text = paragraph.lower()\n",
        "clean_text = clean_text.translate(str.maketrans('', '', string.punctuation))\n",
        "words = word_tokenize(clean_text)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "porter_stems=[porter.stem(word) for word in filtered_words]\n",
        "lancaster_stems = [lancaster.stem(word) for word in filtered_words]\n",
        "print()\n",
        "for word in filtered_words:\n",
        "  print(f\"{word} -> {porter.stem(word)} , {lancaster.stem(word)}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"\\n\")\n",
        "print(\"Lemmatized result is :-\")\n",
        "lematizer = WordNetLemmatizer()\n",
        "for word in filtered_words:\n",
        "  print(f\"{word} , {lematizer.lemmatize(word)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ww7nzBn2ohj7",
        "outputId": "0838c8b9-5933-4195-94a6-feed03823309"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "technology -> technolog , technolog\n",
            "evolving -> evolv , evolv\n",
            "faster -> faster , fast\n",
            "ever -> ever , ev\n",
            "smartphones -> smartphon , smartphon\n",
            "ai -> ai , ai\n",
            "space -> space , spac\n",
            "exploration -> explor , expl\n",
            "robotics -> robot , robot\n",
            "every -> everi , every\n",
            "innovation -> innov , innov\n",
            "shaping -> shape , shap\n",
            "future -> futur , fut\n",
            "simplifies -> simplifi , simpl\n",
            "tasks -> task , task\n",
            "connects -> connect , connect\n",
            "people -> peopl , peopl\n",
            "across -> across , across\n",
            "globe -> globe , glob\n",
            "drives -> drive , driv\n",
            "efficiency -> effici , efficy\n",
            "countless -> countless , countless\n",
            "industries -> industri , industry\n",
            "become -> becom , becom\n",
            "reliant -> reliant , rely\n",
            "digital -> digit , digit\n",
            "systems -> system , system\n",
            "understanding -> understand , understand\n",
            "technology -> technolog , technolog\n",
            "becomes -> becom , becom\n",
            "crucial -> crucial , cruc\n",
            "new -> new , new\n",
            "breakthroughs -> breakthrough , breakthrough\n",
            "continue -> continu , continu\n",
            "amaze -> amaz , amaz\n",
            "transform -> transform , transform\n",
            "way -> way , way\n",
            "live -> live , liv\n",
            "possibilities -> possibl , poss\n",
            "limitless -> limitless , limitless\n",
            "imagination -> imagin , imagin\n",
            "meets -> meet , meet\n",
            "innovation -> innov , innov\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Lemmatized result is :-\n",
            "technology , technology\n",
            "evolving , evolving\n",
            "faster , faster\n",
            "ever , ever\n",
            "smartphones , smartphones\n",
            "ai , ai\n",
            "space , space\n",
            "exploration , exploration\n",
            "robotics , robotics\n",
            "every , every\n",
            "innovation , innovation\n",
            "shaping , shaping\n",
            "future , future\n",
            "simplifies , simplifies\n",
            "tasks , task\n",
            "connects , connects\n",
            "people , people\n",
            "across , across\n",
            "globe , globe\n",
            "drives , drive\n",
            "efficiency , efficiency\n",
            "countless , countless\n",
            "industries , industry\n",
            "become , become\n",
            "reliant , reliant\n",
            "digital , digital\n",
            "systems , system\n",
            "understanding , understanding\n",
            "technology , technology\n",
            "becomes , becomes\n",
            "crucial , crucial\n",
            "new , new\n",
            "breakthroughs , breakthrough\n",
            "continue , continue\n",
            "amaze , amaze\n",
            "transform , transform\n",
            "way , way\n",
            "live , live\n",
            "possibilities , possibility\n",
            "limitless , limitless\n",
            "imagination , imagination\n",
            "meets , meet\n",
            "innovation , innovation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Regular Expressions and Text Spliting\n",
        "\n",
        "Take their original text from Question 1.\n",
        "Use regular expressions to:\n",
        "a. Extract all words with more than 5 letters.\n",
        "\n",
        "b. Extract all numbers (if any exist in their text).\n",
        "\n",
        "c. Extract all capitalized words. 3. Use text spliting techniques to:\n",
        "\n",
        "a. Split the text into words containing only alphabets (removing digits and special characters).\n",
        "\n",
        "b. Extract words starting with a vowel."
      ],
      "metadata": {
        "id": "99glp-EqonId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "paragraph= \"\"\"Technology is evolving faster than ever. From smartphones and AI to space exploration and robotics, every innovation is shaping the future.\n",
        "It simplifies tasks, connects people across the globe, and drives efficiency in countless industries.\n",
        "As we become more reliant on digital systems, understanding technology becomes crucial.\n",
        "New breakthroughs continue to amaze and transform the way we live.\n",
        "The possibilities are limitless when imagination meets innovation.\"\"\"\n",
        "\n",
        "greater_than_five = re.findall(r'\\b[a-zA-Z]{6,}\\b',paragraph)\n",
        "print(greater_than_five)\n",
        "\n",
        "numbers=re.findall(r'\\b\\d+\\b',paragraph)\n",
        "print(numbers)\n",
        "\n",
        "capitalized=re.findall(r'\\b[A-Z][a-zA-Z]*\\b',paragraph)\n",
        "print(capitalized)\n",
        "\n",
        "alphabets=re.findall(r'\\b[a-zA-Z]+\\b',paragraph)\n",
        "print(alphabets)\n",
        "\n",
        "vowels=[word for word in alphabets if re.match(r'^[aeiouAEIOU]',word)]\n",
        "print(vowels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNkUJt1VojP-",
        "outputId": "1522afe4-2cc9-4c0f-8a2d-0bfa86ed0fba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Technology', 'evolving', 'faster', 'smartphones', 'exploration', 'robotics', 'innovation', 'shaping', 'future', 'simplifies', 'connects', 'people', 'across', 'drives', 'efficiency', 'countless', 'industries', 'become', 'reliant', 'digital', 'systems', 'understanding', 'technology', 'becomes', 'crucial', 'breakthroughs', 'continue', 'transform', 'possibilities', 'limitless', 'imagination', 'innovation']\n",
            "[]\n",
            "['Technology', 'From', 'AI', 'It', 'As', 'New', 'The']\n",
            "['Technology', 'is', 'evolving', 'faster', 'than', 'ever', 'From', 'smartphones', 'and', 'AI', 'to', 'space', 'exploration', 'and', 'robotics', 'every', 'innovation', 'is', 'shaping', 'the', 'future', 'It', 'simplifies', 'tasks', 'connects', 'people', 'across', 'the', 'globe', 'and', 'drives', 'efficiency', 'in', 'countless', 'industries', 'As', 'we', 'become', 'more', 'reliant', 'on', 'digital', 'systems', 'understanding', 'technology', 'becomes', 'crucial', 'New', 'breakthroughs', 'continue', 'to', 'amaze', 'and', 'transform', 'the', 'way', 'we', 'live', 'The', 'possibilities', 'are', 'limitless', 'when', 'imagination', 'meets', 'innovation']\n",
            "['is', 'evolving', 'ever', 'and', 'AI', 'exploration', 'and', 'every', 'innovation', 'is', 'It', 'across', 'and', 'efficiency', 'in', 'industries', 'As', 'on', 'understanding', 'amaze', 'and', 'are', 'imagination', 'innovation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Custom Tokenization & Regex-based Text Cleaning\n",
        "\n",
        "Take original text from Question 1.\n",
        "Write a custom tokenization function that:\n",
        "a. Removes punctuation and special symbols, but keeps contractions (e.g., \"isn't\" should not be split into \"is\" and \"n't\").\n",
        "\n",
        "b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains a single token).\n",
        "\n",
        "c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\" should remain as is).\n",
        "\n",
        "Use Regex Substitutions (re.sub) to:\n",
        "a. Replace email addresses with '' placeholder.\n",
        "\n",
        "b. Replace URLs with '' placeholder.\n",
        "\n",
        "c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with '' placeholder."
      ],
      "metadata": {
        "id": "ohdHqzCCouMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "paragraph= \"\"\"Technology is evolving faster than ever. From smartphones and AI to space exploration and robotics, every innovation is shaping the future.\n",
        "It simplifies tasks, connects people across the globe, and drives efficiency in countless industries.\n",
        "As we become more reliant on digital systems, understanding technology becomes crucial.\n",
        "New breakthroughs continue to amaze and transform the way we live.\n",
        "The possibilities are limitless when imagination meets innovation.\"\"\"\n",
        "def custom_tokenize(paragraph):\n",
        "  text=re.sub(r\"[^\\w\\s'-]\",'',paragraph)\n",
        "\n",
        "  text=re.sub(r'(\\w+)[-](\\w+)',r'\\1',paragraph)\n",
        "\n",
        "  text=re.sub(r'(\\d+\\.\\d+|\\d+)',r'\\1',paragraph)\n",
        "\n",
        "  return text.split()\n",
        "\n",
        "tokens=custom_tokenize(paragraph)\n",
        "print(tokens)\n",
        "\n",
        "text_with_sensitive_info = \"\"\"\n",
        "You can reach me at john.doe@example.com or visit https://www.example.com for more details.\n",
        "My phone number is 123-456-7890 or you can contact me via +91 9876543210.\n",
        "\"\"\"\n",
        "def replace_sensitive_info(text):\n",
        "  text=re.sub(r'[a-zA-Z0-9,.%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}','',text)\n",
        "\n",
        "  text=re.sub(r'https?://[^\\s]+','',text)\n",
        "\n",
        "  text=re.sub(r'\\+?\\d{1,3}[\\s\\-]?\\d1,3?[\\s\\-]?\\d{3}[\\s\\-]?\\d{4}','',text)\n",
        "\n",
        "  return text;\n",
        "\n",
        "updated_text = replace_sensitive_info(text_with_sensitive_info)\n",
        "print(updated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnKcSFuvoxij",
        "outputId": "3d10b418-b971-425c-d4df-28be93f1078b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Technology', 'is', 'evolving', 'faster', 'than', 'ever.', 'From', 'smartphones', 'and', 'AI', 'to', 'space', 'exploration', 'and', 'robotics,', 'every', 'innovation', 'is', 'shaping', 'the', 'future.', 'It', 'simplifies', 'tasks,', 'connects', 'people', 'across', 'the', 'globe,', 'and', 'drives', 'efficiency', 'in', 'countless', 'industries.', 'As', 'we', 'become', 'more', 'reliant', 'on', 'digital', 'systems,', 'understanding', 'technology', 'becomes', 'crucial.', 'New', 'breakthroughs', 'continue', 'to', 'amaze', 'and', 'transform', 'the', 'way', 'we', 'live.', 'The', 'possibilities', 'are', 'limitless', 'when', 'imagination', 'meets', 'innovation.']\n",
            "\n",
            "You can reach me at  or visit  for more details.\n",
            "My phone number is 123-456-7890 or you can contact me via +91 9876543210.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}